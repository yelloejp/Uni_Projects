{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "author_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZLNKrHWJAoI"
      },
      "source": [
        "# This notebook includes from building a graph and training GCN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iT65ynQFaiC"
      },
      "source": [
        "### 1) Install required packages\n",
        "To import libaries not existing in Colab, install some packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovmo1a_XaKZU",
        "outputId": "c33620f4-19c1-4ea9-8609-5b8ed3fab381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.9MB 257kB/s \n",
            "\u001b[K     |████████████████████████████████| 24.3MB 118kB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 17.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXRyBvw3JQYT"
      },
      "source": [
        "### 2) Import relevant library\n",
        "Import all libraries required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2CyrNdYISqc",
        "outputId": "e1b4aa51-83fe-445f-9481-97fa73e370f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# For data loading and saving\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from google.colab import files\n",
        "\n",
        "# For data preprocessing\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "\n",
        "# For modeling\n",
        "import torch\n",
        "import scipy.sparse as sp\n",
        "from torch_sparse import coalesce\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PTBSOnROcZH"
      },
      "source": [
        "### 3) Load and preprocess dataset\n",
        "- Read dataset \n",
        "- Remove stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_rru5WfNkUy"
      },
      "source": [
        "train = pd.read_csv('./data/train.csv')\n",
        "test = pd.read_csv('./data/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U90_jzJxYmtq"
      },
      "source": [
        "def alpha_num(text):\n",
        "    return re.sub(r'[^A-Za-z0-9 ]', '', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsPNTsVcN2nj"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "test['author'] = 99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsp6JyaxUbez"
      },
      "source": [
        "train['text'] = train['text'].apply(alpha_num)\n",
        "train['lower'] = train['text'].str.lower()\n",
        "train['tokenized'] = train['lower'].apply(lambda x: word_tokenize(x))\n",
        "train['clean'] = train['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8wZZ8AXVJXx"
      },
      "source": [
        "test['text'] = test['text'].apply(alpha_num)\n",
        "test['lower'] = test['text'].str.lower()\n",
        "test['tokenized'] = test['lower'].apply(lambda x: word_tokenize(x))\n",
        "test['clean'] = test['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV_ygxePUc5n"
      },
      "source": [
        "chosen_idx = np.random.choice(len(train), replace = False, size = int(len(train)*0.3)) \n",
        "train['node_type'] = 1\n",
        "train.iloc[chosen_idx, 6] = 2\n",
        "test['node_type'] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXhmwGLhVg2Z"
      },
      "source": [
        "df = pd.concat([train, test], ignore_index=True)\n",
        "df = df.drop(['index'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIl-nje7VcFk"
      },
      "source": [
        "df = df.drop(['lower','tokenized'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6no3JtVwfb",
        "outputId": "6808f524-abb7-4f03-8d5d-f711a47a59a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>clean</th>\n",
              "      <th>node_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He was almost choking There was so much so muc...</td>\n",
              "      <td>3</td>\n",
              "      <td>[almost, choking, much, much, wanted, say, str...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Your sister asked for it I suppose</td>\n",
              "      <td>2</td>\n",
              "      <td>[sister, asked, suppose]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>She was engaged one day as she walked in peru...</td>\n",
              "      <td>1</td>\n",
              "      <td>[engaged, one, day, walked, perusing, janes, l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The captain was in the porch keeping himself c...</td>\n",
              "      <td>4</td>\n",
              "      <td>[captain, porch, keeping, carefully, way, trea...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have mercy gentlemen odin flung up his hands D...</td>\n",
              "      <td>3</td>\n",
              "      <td>[mercy, gentlemen, odin, flung, hands, dont, w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...  node_type\n",
              "0  He was almost choking There was so much so muc...  ...          1\n",
              "1                 Your sister asked for it I suppose  ...          1\n",
              "2   She was engaged one day as she walked in peru...  ...          1\n",
              "3  The captain was in the porch keeping himself c...  ...          2\n",
              "4  Have mercy gentlemen odin flung up his hands D...  ...          1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uni733_uBUqj"
      },
      "source": [
        "### 4) Train word embeddings\n",
        "- From the dataset, train word2vec embeddings \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGVAyUpk_2Su"
      },
      "source": [
        "sentences = []\n",
        "for sentence in df.clean : \n",
        "  sentences.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWcr_izSS0jC"
      },
      "source": [
        "embeddings = gensim.models.Word2Vec(sentences, iter=50, min_count=30, window=5, size=300, sg=1, negative=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY50t73_GRgv"
      },
      "source": [
        "embs = {x: embeddings.wv[x] for x in embeddings.wv.vocab}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYNUg4lICmD8"
      },
      "source": [
        "### 5) Transform each text into embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h7rBWrOHYE2"
      },
      "source": [
        "df['clean'] = df.clean.apply(lambda x: [item for item in x if item in embeddings.wv.vocab])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEVsEp9bMd70"
      },
      "source": [
        "features = []\n",
        "for doc in range(len(df)):\n",
        "  words = df.clean.iloc[doc]\n",
        "  words = [ embs.get(w) for w in words ]\n",
        "  feature = np.sum(words,axis=0)\n",
        "  features.append(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlYf0tb2d680"
      },
      "source": [
        "### 6) Compute edges(distance between docs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b5D4OceFwdS"
      },
      "source": [
        "edges = np.empty([])\n",
        "rows = np.empty([])\n",
        "cols = np.empty([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvhBGcOub0fd"
      },
      "source": [
        "docs = df.clean\n",
        "for i in range(len(df)):\n",
        "  doc = df.clean.iloc[i]\n",
        "  edge = docs.apply(lambda x: 0 if (len(doc)+len(x)) == 0 else len(set(x).intersection(doc))/((len(doc)+len(x))*0.5))\n",
        "  edge = edge.to_numpy()\n",
        "\n",
        "  val_edge = edge[edge>0.3] \n",
        "  val_num = len(val_edge)\n",
        "  val_row = np.repeat(i, val_num)\n",
        "  val_col = np.where(edge>0.3)[0] \n",
        "\n",
        "  edges = np.append(edges, val_edge)\n",
        "  rows = np.append(rows, val_row)\n",
        "  cols = np.append(cols, val_col)\n",
        "\n",
        "  if i%100 == 0:\n",
        "    print(\"in process: \", i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYgCe1NodkuM"
      },
      "source": [
        "edges = edges[1:]\n",
        "rows = rows[1:]\n",
        "cols = cols[1:]\n",
        "\n",
        "rows = rows.astype('int') \n",
        "cols = cols.astype('int') \n",
        "\n",
        "adj = sp.coo_matrix((edges,(rows,cols)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj00KIGfeQXQ"
      },
      "source": [
        "### 7) Convert to Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TONAd3upd4nR"
      },
      "source": [
        "class MyGraph(InMemoryDataset):\n",
        "\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super(MyGraph, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):  \n",
        "        label = torch.from_numpy(df.author.to_numpy(dtype=np.int32)).to(torch.long)\n",
        "\n",
        "        row = torch.from_numpy(adj.row).to(torch.long)\n",
        "        col = torch.from_numpy(adj.col).to(torch.long)\n",
        "        edge_index = torch.stack([row,col], dim=0)\n",
        "            \n",
        "        edge_index, _ = coalesce(edge_index, None, len(features), len(features))\n",
        "        split = torch.from_numpy(df.node_type.to_numpy(dtype=np.int32))\n",
        "        data = Data(x=features, edge_index=edge_index, y=label)\n",
        "        \n",
        "        data.train_mask = split == 1\n",
        "        data.val_mask = split == 2\n",
        "        data.test_mask = split == 3\n",
        "\n",
        "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD3m_lPpoLJC",
        "outputId": "dac7e7e3-92e4-414a-d557-fda37b271307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = MyGraph(root='./')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_DWMJawMbIX",
        "outputId": "8a35ae1b-ebf3-4bb5-83da-9a091126be7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 10847116], test_mask=[74496], train_mask=[74496], val_mask=[74496], x=[74496, 300], y=[74496])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy3p8wlHolEU",
        "outputId": "959ac23b-107c-4c72-e056-46309f26d914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vZcTHGoor39"
      },
      "source": [
        "data = torch.load('./processed/data.pt')\n",
        "save_path = '/content/drive/My Drive/author.pt'\n",
        "torch.save(data, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B5O6w-do1WM"
      },
      "source": [
        "### 8) Build GraphSage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSA_x0bWozZA"
      },
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QANfDImtoyYN"
      },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSLRtvQtqAwM"
      },
      "source": [
        "dataset = torch.load('./processed/data.pt') \n",
        "data = dataset[0]\n",
        "x = data.x[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bar2hxv_77TM"
      },
      "source": [
        "idx_list = []\n",
        "for idx in range(len(x)):\n",
        "  temp = x[idx]\n",
        "  if temp.shape != (300,) :\n",
        "    idx_list.append(idx)\n",
        "\n",
        "for i in idx_list:\n",
        "  x[i] = np.zeros(300)\n",
        "\n",
        "x = torch.Tensor(x)\n",
        "data.x = x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoUIHcmtsG-Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfeImMRXsHD2"
      },
      "source": [
        "train_loader = NeighborSampler(data.edge_index, node_idx=data.train_mask, sizes=[25, 10], batch_size=1024, shuffle=True, num_workers=12)\n",
        "subgraph_loader = NeighborSampler(data.edge_index, node_idx=None, sizes=[-1], batch_size=1024, shuffle=False, num_workers=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEppuwFesHLV"
      },
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.num_layers = 2\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        for i, (edge_index, _, size) in enumerate(adjs):\n",
        "            x_target = x[:size[1]] \n",
        "            x = self.convs[i]((x, x_target), edge_index)\n",
        "            if i != self.num_layers - 1:\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "    def inference(self, x_all):\n",
        "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
        "        pbar.set_description('Evaluating')\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            xs = []\n",
        "            for batch_size, n_id, adj in subgraph_loader:\n",
        "                edge_index, _, size = adj.to(device)\n",
        "                x = x_all[n_id].to(device)\n",
        "                x_target = x[:size[1]]\n",
        "                x = self.convs[i]((x, x_target), edge_index)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = F.relu(x)\n",
        "                xs.append(x.cpu())\n",
        "\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "            x_all = torch.cat(xs, dim=0)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        return x_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm5JnJopsHQ3"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SAGE(data.num_features, 256, 6)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy8ffbJssHWe"
      },
      "source": [
        "x = data.x.to(device)\n",
        "y = data.y.squeeze().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md6IztxJsHb9"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=int(data.train_mask.sum()))\n",
        "    pbar.set_description(f'Epoch {epoch:02d}')\n",
        "\n",
        "    total_loss = total_correct = 0\n",
        "    for batch_size, n_id, adjs in train_loader:\n",
        "        adjs = [adj.to(device) for adj in adjs]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x[n_id], adjs)\n",
        "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
        "        pbar.update(batch_size)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    loss = total_loss / len(train_loader)\n",
        "    approx_acc = total_correct / int(data.train_mask.sum())\n",
        "\n",
        "    return loss, approx_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbdqQGh7sHhz"
      },
      "source": [
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model.inference(x)\n",
        "    y_true = y.cpu().unsqueeze(-1)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "    results = []\n",
        "    \n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        results += [int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njQz2mZAGeMC"
      },
      "source": [
        "@torch.no_grad()\n",
        "def pred():\n",
        "    model.eval()\n",
        "    out = model.inference(x)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "    results = y_pred[data.test_mask]\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo4wmXuysHkk"
      },
      "source": [
        "for epoch in range(1, 20):\n",
        "    loss, acc = train(epoch)\n",
        "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')   \n",
        "     \n",
        "    train_acc, val_acc, test_acc = test()\n",
        "    print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f} ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpwgSgkfD0uS"
      },
      "source": [
        "results = pred()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYsNcYniHNUD"
      },
      "source": [
        "results = torch.nn.functional.one_hot(results, num_classes=-1)\n",
        "results = results.numpy()\n",
        "results = results.reshape(19617, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTL74_OjKK95"
      },
      "source": [
        "results = pd.DataFrame(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYlPfsXOLVTt"
      },
      "source": [
        "results.to_csv('./data/results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
