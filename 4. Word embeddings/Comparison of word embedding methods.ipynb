{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for module paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code contains 4 tasks: 1) creating word paris for intrinsic evaluation, 2) Using 3 different word embeddings, 3) evaluating these embeddings on intrinsic and 4) extrinsic evaluation methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Creating word pairs \n",
    "\n",
    "It creates a set of 200 word pairs from our corpus. It excludes words occured frequently and occured less than 30 times. For the second task, it will uses words existing in pre-trained embedding method. \n",
    "By utilizing WordNet, it creates a set of word pairs. WordNet has synsets of noun, verb, adverb and adjective. It collects only nouns out of 4 pos-tagging. For the similarity of two nouns, it computes all similarities of synset pairs. For example, the first word \"boat\" has two synsets and the second word \"ship\" has one synset. It averages the similarities of boat-ship and gravy_boat-ship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boat synsets: [Synset('boat.n.01'), Synset('gravy_boat.n.01')]\n",
      "Ship synsets: [Synset('ship.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "print(\"Boat synsets:\",wn.synsets('boat', pos='n'))\n",
    "print(\"Ship synsets:\",wn.synsets('ship', pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Using 3 different word embeddings\n",
    "\n",
    "It trains Skip-gram and CBOW methods and uses a pre-trained method. As the pre-trained method, it uses GoogleNews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Evaluating intrinsic method \n",
    "\n",
    "It evaluates abilities of 3 different word embeddings on the set of word pairs created in task 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Evaluating extrinsic method \n",
    "\n",
    "The 3 different word embeddings in task 3 are used for text classification problem. The corpus consists of 18 datasets collected by 18 participants. Each dataset has label. The goal of text classification is to predict this label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Reading the Corpus\n",
    "\n",
    "The texts in the corpus are already split into sentences. It reads these sentences and tokenizes each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging_level = logging.DEBUG\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging_level)\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def load_data(filename):\n",
    "    return json.loads(gzip.GzipFile(filename).read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_corpus = load_data('./material/nlpcm_corpus_1.json.gz')\n",
    "logging.info(' The course corpus consists of %d subcorpora:' % len(course_corpus))\n",
    "\n",
    "index = 0\n",
    "for designer, texts in course_corpus.items():\n",
    "    logging.info(' %d: %6d texts gathered by %s with %d characters in total.' % (index, len(texts), designer, sum([len(text) for text in texts])))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Tokenizing the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text of subcorpus 0 of 18\n",
      "Tokenizing text of subcorpus 1 of 18\n",
      "Tokenizing text of subcorpus 2 of 18\n",
      "Tokenizing text of subcorpus 3 of 18\n",
      "Tokenizing text of subcorpus 4 of 18\n",
      "Tokenizing text of subcorpus 5 of 18\n",
      "Tokenizing text of subcorpus 6 of 18\n",
      "Tokenizing text of subcorpus 7 of 18\n",
      "Tokenizing text of subcorpus 8 of 18\n",
      "Tokenizing text of subcorpus 9 of 18\n",
      "Tokenizing text of subcorpus 10 of 18\n",
      "Tokenizing text of subcorpus 11 of 18\n",
      "Tokenizing text of subcorpus 12 of 18\n",
      "Tokenizing text of subcorpus 13 of 18\n",
      "Tokenizing text of subcorpus 14 of 18\n",
      "Tokenizing text of subcorpus 15 of 18\n",
      "Tokenizing text of subcorpus 16 of 18\n",
      "Tokenizing text of subcorpus 17 of 18\n",
      "Corpus number of sentences: 658902\n",
      "Corpus number of tokens: 14548903\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentences = []\n",
    "index = 0\n",
    "for designer, texts in course_corpus.items():\n",
    "    print('Tokenizing text of subcorpus %d of %d' % (index, len(course_corpus)))\n",
    "    index += 1\n",
    "    for text in texts:\n",
    "        for sentence in nltk.sent_tokenize(text, language=\"english\"):\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence, language=\"english\")\n",
    "            sentences.append(tokenized_sentence)\n",
    "print('Corpus number of sentences: %d' % len(sentences))\n",
    "print('Corpus number of tokens: %d' % sum([len(sentence) for sentence in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Preprocessing\n",
    "\n",
    "Instead of using all words occured in the corpus, it preprocesses the text to remove some words which are not helpful such as stopwords. According to Zipf's law, some terms like the, a, etc. occur often in a corpus but not helpful for NLP tasks. The top 100 words that occur often cover 50% of words. For the task, it excludes these words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "wortfrequenz = Counter()\n",
    "\n",
    "for satz in sentences:\n",
    "    wortfrequenz.update(satz)\n",
    "\n",
    "vocabulary = [w for w,f in wortfrequenz.items() if 30 < f < 19000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sorted_words = sorted(wortfrequenz, key=lambda word: wortfrequenz[word], reverse=True)\n",
    "word_ranks = {word: rank+1 for rank, word in enumerate(sorted_words)}\n",
    "frequency_ranks = {word_ranks[word]: frequency for word, frequency in wortfrequenz.items()}\n",
    "\n",
    "lists = sorted(frequency_ranks.items())\n",
    "x, y = zip(*lists)\n",
    "\n",
    "half_sum = 0\n",
    "stop_rank = 0\n",
    "for rank in range(len(y)):\n",
    "    if half_sum <= sum(wortfrequenz.values())*0.5 :\n",
    "        half_sum += y[rank]\n",
    "        stop_rank = rank\n",
    "    else :         \n",
    "        break\n",
    "print(\"{0} words cover 50% of words. Its sum is {1}\".format(stop_rank+1, half_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_limit = 100\n",
    "plt.plot(x[:n_limit], y[:n_limit], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Collecting nouns\n",
    "\n",
    "It creates a noun word pairs for evaluation. By utilizing, it can extract nouns from our vocabulary which exist in WordNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "filtered_nouns = vocabulary\n",
    "\n",
    "for pos in ['a','s','v','r']:\n",
    "    pos_set = list(set(chain(*[i.lemma_names() for i in wn.all_synsets(pos)])))\n",
    "    pos_set = [x.lower() for x in pos_set]\n",
    "    filtered_nouns = [w for w in filtered_nouns if w.lower() not in pos_set]\n",
    "\n",
    "noun_set = list(set(chain(*[i.lemma_names() for i in wn.all_synsets('n')])))\n",
    "noun_set = [x.lower() for x in noun_set]\n",
    "filtered_nouns = [w for w in filtered_nouns if w.lower() in noun_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words tokenized : 14548903\n",
      "The number of words without stopwords : 18099\n",
      "The number of nouns: 4954\n"
     ]
    }
   ],
   "source": [
    "print('The number of words tokenized : %d' % sum([len(sentence) for sentence in sentences]))\n",
    "print('The number of words without stopwords : %d' % len(vocabulary))\n",
    "print('The number of nouns: %d' % len(filtered_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Computing similarity for all noun pairs \n",
    "\n",
    "It uses Wu-Palmer similarity which returns a score how similar two word senses are based on the depth of two senses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_synsets(word):\n",
    "    return len(wn.synsets(word, pos='n'))\n",
    "\n",
    "def wup_similarity(word1, word2):\n",
    "    return word1.wup_similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_nouns = len(filtered_nouns)\n",
    "\n",
    "pairs = []\n",
    "for i in range(Num_nouns) : \n",
    "    for j in range(Num_nouns) : \n",
    "        if i != j :\n",
    "            w1, w2 = filtered_nouns[i], filtered_nouns[j]\n",
    "            N, M = num_synsets(w1), num_synsets(w2)\n",
    "            \n",
    "            dist = 0\n",
    "            for x in range(N) :\n",
    "                word1 = wn.synsets(w1, pos='n')[x:x+1][0]\n",
    "                for y in range(M) :\n",
    "                    word2 = wn.synsets(w2, pos='n')[y:y+1][0]\n",
    "                    dist += wup_similarity(word1, word2)\n",
    "            dist = dist / (N*M)\n",
    "            pair = [w1, w2, dist]\n",
    "            pairs.append(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6) Removing words not in pre-trained method: GoogleNews\n",
    "\n",
    "It removes a word pair where the first and second word are same. Moreover, it excludes word pairs not in GoogleNews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "DEBUG:{'uri': 'GoogleNews-vectors-negative300.bin.gz', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'transport_params': None}\n",
      "INFO:loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "pre_trained = [w for w,f in word2vec.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs: 24082208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pairs, columns=['word1', 'word2', 'similarity']) \n",
    "df = df.loc[df['word1'].str.lower()!=df['word2'].str.lower()]\n",
    "df = df[df['word1'].isin(pre_trained) & df['word2'].isin(pre_trained)]\n",
    "print('The number of pairs: %d' % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7) Sampling 200 word pairs \n",
    "\n",
    "It samples 100 pairs which similarities are bigger than 0.5 and the rest less than 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_pairs = pd.concat((df[df.similarity>=0.5].sample(100),df[df.similarity<0.5].sample(100))).reset_index()\n",
    "set_pairs = set_pairs.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scotland</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capo</td>\n",
       "      <td>da</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ordeal</td>\n",
       "      <td>Keeping</td>\n",
       "      <td>0.524269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>panty</td>\n",
       "      <td>classroom</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benny</td>\n",
       "      <td>Einstein</td>\n",
       "      <td>0.600752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1      word2  similarity\n",
       "0  Scotland    Detroit    0.700000\n",
       "1      capo         da    0.545455\n",
       "2    ordeal    Keeping    0.524269\n",
       "3     panty  classroom    0.500000\n",
       "4     Benny   Einstein    0.600752"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8) 3 different word embeddings \n",
    "\n",
    "Here, it trains skip_gram and CBOW word embedding methods and reads pre-trained word embedding with GoogleNews dataset. For skip_gram and CBOW, it iterates its training 30 times and sets window = 5. The dimension of word embeddings in three models is 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "skip_gram = gensim.models.Word2Vec(sentences, iter=30, min_count=30, window=5, size=300, sg=1, negative=20)\n",
    "CBOW = gensim.models.Word2Vec(sentences, iter=30, min_count=30, window=5, size=300, sg=0, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "def mag(x):\n",
    "    return np.sqrt(x.dot(x))\n",
    "\n",
    "vectors_skip_gram = {w:skip_gram.wv[w]/mag(skip_gram.wv[w]) for w in vocabulary}\n",
    "vectors_CBOW = {w:CBOW.wv[w]/mag(CBOW.wv[w]) for w in vocabulary }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a word \"love\", word representations look like following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram:  [-0.05764098 -0.05651434 -0.02088199  0.05422454 -0.05084962]\n",
      "CBOW:  [ 0.10642056 -0.01309693 -0.04140827  0.00130632  0.02166286]\n",
      "Pre-trained:  [ 0.10302734 -0.15234375  0.02587891  0.16503906 -0.16503906]\n"
     ]
    }
   ],
   "source": [
    "print('Skip-gram: ', vectors_skip_gram['love'][0:5])\n",
    "print('CBOW: ', vectors_CBOW['love'][0:5])\n",
    "print('Pre-trained: ', word2vec.word_vec('love')[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9) Intrinsic evaluation \n",
    "\n",
    "By using the set of word pairs in task 1, it evaluates abilities of three word embedding methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def evaluate(data, vectors):\n",
    "    gold = []\n",
    "    predicted = []\n",
    "    for v,w,sim in data:\n",
    "        try :\n",
    "            pred = vectors[v].dot(vectors[w])\n",
    "        except TypeError : \n",
    "            pred = vectors(v).dot(vectors(w))\n",
    "        gold.append(sim)\n",
    "        predicted.append(pred)\n",
    "    \n",
    "    av_p = sum(predicted)/len(predicted)\n",
    "    av_g = sum(gold)/len(gold)\n",
    "    \n",
    "    cov = 0\n",
    "    var_g = 0\n",
    "    var_p = 0\n",
    "    for s,t in zip(gold,predicted):\n",
    "        cov += (s-av_g) * (t-av_p)\n",
    "        var_g += (s-av_g) * (s-av_g)\n",
    "        var_p += (t-av_p) * (t-av_p)\n",
    "        \n",
    "    return cov / math.sqrt(var_g*var_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['word1'].isin(vocabulary) & df['word2'].isin(vocabulary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram:  0.30420823489512194\n",
      "CBOW:  0.4031173769615571\n",
      "Pre-trained:  0.3882292940473257\n"
     ]
    }
   ],
   "source": [
    "set_pairs = pd.concat((df[df.similarity>=0.5].sample(100),df[df.similarity<0.5].sample(100))).reset_index()\n",
    "set_pairs = set_pairs.drop(['index'], axis=1)\n",
    "set_pairs.to_csv('test.csv', index=False) \n",
    "\n",
    "val_skip_gram = evaluate(set_pairs.values.tolist(), vectors_skip_gram)\n",
    "val_CBOW = evaluate(set_pairs.values.tolist(), vectors_CBOW)\n",
    "val_pre_trained = evaluate(set_pairs.values.tolist(), word2vec.word_vec)\n",
    "\n",
    "print('Skip-gram: ', val_skip_gram)\n",
    "print('CBOW: ', val_CBOW)\n",
    "print('Pre-trained: ', val_pre_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10) Text classification \n",
    "\n",
    "For the task, several steps are needed: 1) creating index for each word, 2) creating instances only containing index, 3) labeling 18 datasets from different participants and 4) Spliting training and testing sets. With above preparation, it is ready to build neural network for text classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "embedding_matrix = np.random.uniform(-1, 1, (len(word_index) + len(skip_gram.wv.vocab), 300)) ## change vocab to each embeddings. \n",
    "for word in skip_gram.wv.vocab:\n",
    "    index = len(word_index)\n",
    "    word_index[word] = index\n",
    "    embedding_matrix[index] = skip_gram.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_min_length = 10\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "instances = {designer: [] for designer in course_corpus}\n",
    "for designer, texts in course_corpus.items():\n",
    "    logging.info('Processing texts of designer %s; %d instances so far.' % (designer, len(instances)))\n",
    "    for text in texts:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            if len(tokens) >= sentence_min_length:\n",
    "                instances[designer].append([word_index.get(word, word_index[\"<UNK>\"]) for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "TEST_TRAIN_RATIO = 0.1\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30 \n",
    "\n",
    "def pad_input(sentences):\n",
    "    return keras.preprocessing.sequence.pad_sequences(\n",
    "        sentences, maxlen=MAX_SEQUENCE_LENGTH, dtype='int32', padding='pre', truncating='pre', value=word_index[\"<PAD>\"])\n",
    "\n",
    "train_labeled_data = []\n",
    "test_labeled_data = []\n",
    "designer_index = {}\n",
    "for designer, designer_instances in instances.items():\n",
    "    designer_index[designer] = len(designer_index)\n",
    "    random.shuffle(designer_instances)\n",
    "    test_labeled_data += [(inst, designer_index[designer]) for inst in designer_instances[:round(len(designer_instances)*TEST_TRAIN_RATIO)]]\n",
    "    train_labeled_data += [(inst, designer_index[designer]) for inst in designer_instances[round(len(designer_instances)*TEST_TRAIN_RATIO):]]\n",
    "\n",
    "random.shuffle(train_labeled_data)\n",
    "train_data = pad_input([inst[0] for inst in train_labeled_data])\n",
    "train_labels = [inst[1] for inst in train_labeled_data]\n",
    "\n",
    "random.shuffle(test_labeled_data)\n",
    "test_data = pad_input([inst[0] for inst in test_labeled_data])\n",
    "test_labels = [inst[1] for inst in test_labeled_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(len(word_index),\n",
    "                                         300,\n",
    "                                         weights=[embedding_matrix],\n",
    "                                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                         trainable=False)(input_layer)\n",
    "text_encoder = keras.layers.LSTM(64,\n",
    "                                 dropout=0.2,\n",
    "                                 recurrent_dropout=0.2)(embedding_layer)\n",
    "\n",
    "output_layer = keras.layers.Dense(len(course_corpus),\n",
    "                                  activation='softmax')(text_encoder)\n",
    "\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model_loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(loss=model_loss,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(train_data),\n",
    "          np.array(train_labels),\n",
    "          epochs=100,\n",
    "          batch_size = 512,\n",
    "          verbose=1)\n",
    "\n",
    "model.evaluate(np.array(test_data), np.array(test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
